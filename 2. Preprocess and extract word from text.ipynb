{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import re\n",
    "import plotly.express as px\n",
    "from spacy import displacy\n",
    "from spacy.symbols import NOUN, DET, ADJ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download nl_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "spacy.load('nl_core_news_sm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "spacy.load('nl_core_news_md') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "spacy.load('nl_core_news_lg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('nl_core_news_md') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/processed/df_wiki_text.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"De deur is nu gesloten, in het slot gevallen.Je bent er doorheen gegaan.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        \n",
    "        print(token.text, token.ent_type_, token.sentiment, [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_de_of_het(doc):\n",
    "    substantieven = []\n",
    " \n",
    "    for token in doc:\n",
    "\n",
    "    #     print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "    #             [child for child in token.children])\n",
    "\n",
    "\n",
    "        if token.pos == NOUN:\n",
    "            # Case 1 : If there is het of de in the POS then extract the head text\n",
    "    #         print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "    #             [child for child in token.children])\n",
    "\n",
    "\n",
    "            # Case 2 : There is no clear DET. There must be some ADJ before the noun.\n",
    "\n",
    "            # if the last letter of the adjective is 'e' and  then DET is always de\n",
    "            children = [child for child in token.children]\n",
    "            has_adjective = any([child.pos == ADJ for child in token.children])\n",
    "            has_det = any([child.pos == DET for child in token.children])\n",
    "            has_de_of_het = any([child.lemma_ == 'de' or  child.lemma_ == 'het' for child in token.children])\n",
    "            has_geen_of_een = any([child.lemma_ == 'geen' or  child.lemma_ == 'een' for child in token.children])\n",
    "            if has_adjective or has_det:\n",
    "                found_adj_with_last_letter_e=False\n",
    "\n",
    "                if not has_de_of_het and has_adjective and has_geen_of_een:\n",
    "\n",
    "                    for child in children:\n",
    "                        if child.pos == ADJ:\n",
    "                            if child.text[-1] == 'e':\n",
    "                                substantieven.append(  \n",
    "                                    {'det': 'de',\n",
    "                                     \n",
    "                                     'woord' : token.text,\n",
    "                                     'woord_vec':token.vector\n",
    "                                    } )\n",
    "                                found_adj_with_last_letter_e=True\n",
    "                                break\n",
    "                    if not found_adj_with_last_letter_e:\n",
    "\n",
    "                        substantieven.append( {'det': 'het','woord' : token.text,'woord_vec':token.vector} )\n",
    "#                     print(f\"Token text  {token.text}\")     \n",
    "#                     print(f\"Token children  {children}\")     \n",
    "#                     print(f\"Found adjective ending with 'e' :{found_adj_with_last_letter_e}\" )\n",
    "\n",
    "                if has_de_of_het and not has_geen_of_een :\n",
    "\n",
    "                    for child in children:\n",
    "                        if child.pos == DET:\n",
    "                            substantieven.append( {'det': child.lemma_,'woord' : token.text,'woord_vec':token.vector} )\n",
    "                            break\n",
    "\n",
    "        \n",
    "                \n",
    "    \n",
    "    return substantieven\n",
    "\n",
    "\n",
    "\n",
    "    #         if token.lemma_[-1] == 'e': \n",
    "    #             substantieven.append({'det': 'het','woord' : token.head.lemma_})\n",
    "    #         else : # Else it is a de woord\n",
    "    #             substantieven.append({'det': 'de','woord' : token.head.lemma_})\n",
    "\n",
    "def is_word_det_or_het(word):\n",
    "    return word=='de' or word=='het'   \n",
    "\n",
    "def assert_that_only_wanted_articles_are_added(list_of_articles):\n",
    "    for article in list_of_articles:\n",
    "        assert is_word_det_or_het(article['det']), f\"The extraction script found article other than het or de. It found {article['det']}\"\n",
    "    \n",
    "    \n",
    "def extract_de_of_het_simpler(doc):\n",
    "    substantieven = []\n",
    " \n",
    "    for token in doc:\n",
    "\n",
    "    #     print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "    #             [child for child in token.children])\n",
    "\n",
    "\n",
    "        if token.pos == NOUN:\n",
    "            # Case 1 : If there is het of de in the POS then extract the head text\n",
    "    #         print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "    #             [child for child in token.children])\n",
    "\n",
    "\n",
    "            # Case 2 : There is no clear DET. There must be some ADJ before the noun.\n",
    "\n",
    "            # if the last letter of the adjective is 'e' and  then DET is always de\n",
    "            children = [child for child in token.children]\n",
    "            \n",
    "            if len(children)>0:\n",
    "\n",
    "                for child in children:\n",
    "                    if is_word_det_or_het(child.lemma_):\n",
    "                        \n",
    "                        substantieven.append( \n",
    "                            {\n",
    "                            'det': child.lemma_,\n",
    "                            'woord' : token.text,\n",
    "                            'woord_vec':token.vector\n",
    "                                                  \n",
    "                            } )\n",
    "    \n",
    "    # Make sure that \n",
    "    assert_that_only_wanted_articles_are_added(substantieven)\n",
    "    \n",
    "    return substantieven\n",
    "\n",
    "\n",
    "\n",
    "def pre_process_text_generator(text_list):\n",
    "    \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    for text in text_list:\n",
    "        if isinstance(text, str):\n",
    "            text = text.lower()                 # Converting to lowercase\n",
    "\n",
    "            text = re.sub(cleanr, ' ', text)                 # Removing HTML tags\n",
    "            text = re.sub(r'[?|!|\\'|\"|#]',r'',text)\n",
    "            text = re.sub(r'[.|,|)|(|\\|/]',r' ',text)        # Removing Punctuations\n",
    "\n",
    "            yield text\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = pre_process_text_generator(df.text.tolist())\n",
    "\n",
    "substantieven_doc_wise = []\n",
    "with nlp.disable_pipes([\"ner\"],):\n",
    "    print(f\"Using pipeline : {nlp.pipe_names}\")\n",
    "    for doc in nlp.pipe(cleaned_text,n_process=4,batch_size=100):\n",
    "        \n",
    "        lidwoordenlijst = extract_de_of_het_simpler(doc)\n",
    "        \n",
    "        substantieven_doc_wise.append(lidwoordenlijst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(substantieven_doc_wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = pd.DataFrame.from_dict(flatten(substantieven_doc_wise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from name\n",
    "df_processed.drop_duplicates(subset='woord',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(df_processed.woord.apply(lambda w : len(w)>0 and isinstance(w,str))) # Make sure that there are actual words and that each word is a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.det.value_counts()/df_processed.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_pickle('./data/processed/woorden_met_hetofde.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('datascience': conda)",
   "language": "python",
   "name": "python37464bitdatasciencecondaa6d3a21c00574213ab53e74f2ef82285"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
